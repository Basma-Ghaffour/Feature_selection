
@article{genuer_vsurf:_2015,
	title = {{VSURF}: {An} {R} {Package} for {Variable} {Selection} {Using} {Random} {Forests}},
	volume = {7},
	shorttitle = {{VSURF}},
	language = {en},
	number = {2},
	journal = {The R Journal},
	author = {Genuer, Robin and Poggi, Jean-Michel and Tuleau-Malot, Christine},
	year = {2015},
	pages = {19},
}

@book{khun_feature_2019,
	title = {Feature {Engineering} and {Selection}: {A} {Practical} {Approach} for {Predictive} {Models}},
	author = {Khun, Max and Johnson, Kjell},
	month = jun,
	year = {2019},
}

@book{kuhn_applied_2013,
	address = {New York},
	title = {Applied predictive modeling},
	isbn = {9781461468486},
	publisher = {Springer},
	author = {Kuhn, Max and Johnson, Kjell},
	year = {2013},
	note = {OCLC: ocn827083441},
	keywords = {Mathematical statistics, Mathematical models, Prediction theory, Mathematical models, Mathematical statistics, Prediction theory},
}

@book{hastie_elements_2009,
	address = {New York, NY},
	edition = {2nd ed},
	series = {Springer series in statistics},
	title = {The elements of statistical learning: data mining, inference, and prediction},
	isbn = {9780387848570 9780387848587},
	shorttitle = {The elements of statistical learning},
	publisher = {Springer},
	author = {Hastie, Trevor and Tibshirani, Robert and Friedman, J. H.},
	year = {2009},
	keywords = {Machine learning, Statistics, Methodology, Data mining, Bioinformatics, Inference, Forecasting, Computational intelligence},
}

@article{ghattas_importance_1999,
	title = {Importance des variables dans les méthodes {CART}},
	journal = {MODULAD},
	author = {Ghattas, Badih},
	year = {1999},
	pages = {29--39},
}

@article{genuer_variable_2010,
	title = {Variable selection using random forests},
	volume = {31},
	issn = {0167-8655},
	url = {https://www.sciencedirect.com/science/article/pii/S0167865510000954},
	doi = {10.1016/j.patrec.2010.03.014},
	abstract = {This paper proposes, focusing on random forests, the increasingly used statistical method for classification and regression problems introduced by Leo Breiman in 2001, to investigate two classical issues of variable selection. The first one is to find important variables for interpretation and the second one is more restrictive and try to design a good parsimonious prediction model. The main contribution is twofold: to provide some experimental insights about the behavior of the variable importance index based on random forests and to propose a strategy involving a ranking of explanatory variables using the random forests score of importance and a stepwise ascending variable introduction strategy.},
	language = {en},
	number = {14},
	urldate = {2023-07-07},
	journal = {Pattern Recognition Letters},
	author = {Genuer, Robin and Poggi, Jean-Michel and Tuleau-Malot, Christine},
	month = oct,
	year = {2010},
	keywords = {Random forests, Regression, Classification, Variable importance, Variable selection, High dimensional data},
	pages = {2225--2236},
}

@inproceedings{genuer_vsurf_2014,
	title = {{VSURF} : un package {R} pour la sélection de variables à l'aide de forêts aléatoires},
	shorttitle = {{VSURF}},
	url = {https://inria.hal.science/hal-01096233},
	abstract = {This paper describes the R package VSURF. Based on random forests, it delivers two subsets of variables according to two types of variable selection for clas-sification or regression problems. The first is a subset of important variables which are relevant for interpretation, while the second one is a subset corresponding to a parsimo-nious prediction model. The strategy is based on a preliminary ranking of the explanatory variables using the random forests permutation-based score of importance and proceeds using a stepwise ascending variable introduction strategy. The two proposals can be ob-tained automatically using data-driven default values, good enough to provide interesting results, but can also be fine-tuned by the user. The algorithm is illustrated on a simulated example and its applications to real datasets are presented.},
	language = {en},
	urldate = {2023-07-07},
	author = {Genuer, Robin and Poggi, Jean-Michel and Tuleau-Malot, Christine},
	year = {2014},
}

@misc{leo_breiman_breiman_2022,
	title = {Breiman and {Cutler}'s {Random} {Forests} for {Classification} and {Regression}},
	shorttitle = {Package ‘{randomForest}’},
	url = {https://cran.r-project.org/web/packages/randomForest/},
	language = {English},
	publisher = {2022-05-23},
	author = {{Leo Breiman} and Cutler, Adele and Liaw, Andy and Wiener, Matthew},
	month = jan,
	year = {2022},
}

@article{strobl_conditional_2008,
	title = {Conditional variable importance for random forests},
	volume = {9},
	issn = {1471-2105},
	url = {https://doi.org/10.1186/1471-2105-9-307},
	doi = {10.1186/1471-2105-9-307},
	abstract = {Random forests are becoming increasingly popular in many scientific fields because they can cope with "small n large p" problems, complex interactions and even highly correlated predictor variables. Their variable importance measures have recently been suggested as screening tools for, e.g., gene expression studies. However, these variable importance measures show a bias towards correlated predictor variables.},
	number = {1},
	urldate = {2023-07-12},
	journal = {BMC Bioinformatics},
	author = {Strobl, Carolin and Boulesteix, Anne-Laure and Kneib, Thomas and Augustin, Thomas and Zeileis, Achim},
	month = jul,
	year = {2008},
	keywords = {Random Forest, Importance Measure, Importance Score, Amino Acid Property, Permutation Scheme},
	pages = {307},
}

@article{strobl_bias_2007,
	title = {Bias in random forest variable importance measures: {Illustrations}, sources and a solution},
	volume = {8},
	issn = {1471-2105},
	shorttitle = {Bias in random forest variable importance measures},
	url = {https://doi.org/10.1186/1471-2105-8-25},
	doi = {10.1186/1471-2105-8-25},
	abstract = {Variable importance measures for random forests have been receiving increased attention as a means of variable selection in many classification tasks in bioinformatics and related scientific fields, for instance to select a subset of genetic markers relevant for the prediction of a certain disease. We show that random forest variable importance measures are a sensible means for variable selection in many applications, but are not reliable in situations where potential predictor variables vary in their scale of measurement or their number of categories. This is particularly important in genomics and computational biology, where predictors often include variables of different types, for example when predictors include both sequence data and continuous variables such as folding energy, or when amino acid sequence data show different numbers of categories.},
	number = {1},
	urldate = {2023-07-12},
	journal = {BMC Bioinformatics},
	author = {Strobl, Carolin and Boulesteix, Anne-Laure and Zeileis, Achim and Hothorn, Torsten},
	month = jan,
	year = {2007},
	keywords = {Variable Selection, Random Forest, Bootstrap Sampling, Variable Importance, Importance Measure},
	pages = {25},
}

@article{miao_survey_2016,
	title = {A {Survey} on {Feature} {Selection}},
	journal = {Procedia Computer Science},
	author = {Miao, Jianyu and Niu, Lingfeng},
	year = {2016},
}

@article{chandrashekar_survey_2014,
	series = {40th-year commemorative issue},
	title = {A survey on feature selection methods},
	volume = {40},
	issn = {0045-7906},
	url = {https://www.sciencedirect.com/science/article/pii/S0045790613003066},
	doi = {10.1016/j.compeleceng.2013.11.024},
	abstract = {Plenty of feature selection methods are available in literature due to the availability of data with hundreds of variables leading to data with very high dimension. Feature selection methods provides us a way of reducing computation time, improving prediction performance, and a better understanding of the data in machine learning or pattern recognition applications. In this paper we provide an overview of some of the methods present in literature. The objective is to provide a generic introduction to variable elimination which can be applied to a wide array of machine learning problems. We focus on Filter, Wrapper and Embedded methods. We also apply some of the feature selection techniques on standard datasets to demonstrate the applicability of feature selection techniques.},
	language = {en},
	number = {1},
	urldate = {2023-07-13},
	journal = {Computers \& Electrical Engineering},
	author = {Chandrashekar, Girish and Sahin, Ferat},
	month = jan,
	year = {2014},
	pages = {16--28},
}

@article{smith_step_2018,
	title = {Step away from stepwise},
	volume = {5},
	issn = {2196-1115},
	url = {https://doi.org/10.1186/s40537-018-0143-6},
	doi = {10.1186/s40537-018-0143-6},
	abstract = {Stepwise regression is a popular data-mining tool that uses statistical significance to select the explanatory variables to be used in a multiple-regression model.},
	number = {1},
	urldate = {2023-07-13},
	journal = {Journal of Big Data},
	author = {Smith, Gary},
	month = sep,
	year = {2018},
	keywords = {Stepwise regression, Data mining, Big Data},
	pages = {32},
}

@article{derksen_backward_1992,
	title = {Backward, forward and stepwise automated subset selection algorithms: {Frequency} of obtaining authentic and noise variables},
	volume = {45},
	issn = {00071102},
	shorttitle = {Backward, forward and stepwise automated subset selection algorithms},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/j.2044-8317.1992.tb00992.x},
	doi = {10.1111/j.2044-8317.1992.tb00992.x},
	language = {en},
	number = {2},
	urldate = {2023-07-13},
	journal = {British Journal of Mathematical and Statistical Psychology},
	author = {Derksen, Shelley and Keselman, H. J.},
	month = nov,
	year = {1992},
	pages = {265--282},
}



@misc{misc_student_performance_320,
  author       = {Cortez,Paulo},
  title        = {{Student Performance}},
  url          = {https://archive.ics.uci.edu/dataset/320/student+performance},
  year         = {2014},
  howpublished = {UCI Machine Learning Repository},
  note         = {{DOI}: https://doi.org/10.24432/C5TG7T}
}

@misc{misc_sml2010_274,
  author       = {Romeu-Guallart,Pablo and Zamora-Martinez,Francisco},
  title        = {{SML2010}},
  url          = {https://archive.ics.uci.edu/dataset/274/sml2010},
  year         = {2014},
  howpublished = {UCI Machine Learning Repository},
  note         = {{DOI}: https://doi.org/10.24432/C5RS3S}
}



@article{strobl_introduction_2009,
	title = {An {Introduction} to {Recursive} {Partitioning}: {Rationale}, {Application} and {Characteristics} of {Classification} and {Regression} {Trees}, {Bagging} and {Random} {Forests}},
	volume = {14},
	issn = {1082-989X},
	shorttitle = {An {Introduction} to {Recursive} {Partitioning}},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2927982/},
	doi = {10.1037/a0016973},
	abstract = {Recursive partitioning methods have become popular and widely used tools for non-parametric regression and classification in many scientific fields. Especially random forests, that can deal with large numbers of predictor variables even in the presence of complex interactions, have been applied successfully in genetics, clinical medicine and bioinformatics within the past few years., High dimensional problems are common not only in genetics, but also in some areas of psychological research, where only few subjects can be measured due to time or cost constraints, yet a large amount of data is generated for each subject. Random forests have been shown to achieve a high prediction accuracy in such applications, and provide descriptive variable importance measures reflecting the impact of each variable in both main effects and interactions., The aim of this work is to introduce the principles of the standard recursive partitioning methods as well as recent methodological improvements, to illustrate their usage for low and high dimensional data exploration, but also to point out limitations of the methods and potential pitfalls in their practical application., Application of the methods is illustrated using freely available implementations in the R system for statistical computing.},
	number = {4},
	urldate = {2023-07-21},
	journal = {Psychological methods},
	author = {Strobl, Carolin and Malley, James and Tutz, Gerhard},
	month = dec,
	year = {2009},
	pmid = {19968396},
	pmcid = {PMC2927982},
	pages = {323--348},
}


@inproceedings{hall_feature_1997,
	title = {Feature {Subset} {Selection}: {A} {Correlation} {Based} {Filter} {Approach}},
	shorttitle = {Feature {Subset} {Selection}},
	url = {https://www.semanticscholar.org/paper/Feature-Subset-Selection%3A-A-Correlation-Based-Hall-Smith/c5491cc42ec80a59e843790393e2b6117efd78a1},
	abstract = {Recent work has shown that feature subset selection can have a positive affect on the performance of machine learning algorithms. Some algorithms can be slowed or their performance irrelevant or redundant to the learning task. Feature subset selection, then, is a method for enhancing the performance of learning algorithms, reducing the hypothesis search space, and, in some cases, reducing the storage requirement. This paper describes a feature subset selector that uses a correlation based evaluates its effectiveness with three common ML algorithms: a decision tree inducer (C4.5), a naive Bayes classifier, and an instance based learner (IB1). Experiments using a number of standard data sets drawn from real and artificial domains are presented. Feature subset selection gave significant improvement for all three algorithms; C4.5 generated smaller decision trees.},
	urldate = {2023-07-28},
	author = {Hall, M. and Smith, L. A.},
	year = {1997},
}

